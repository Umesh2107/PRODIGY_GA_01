# ğŸ§  GPT-2 Text Generation with Fine-Tuning

This project demonstrates how to fine-tune the GPT-2 language model using a small custom dataset to generate human-like text based on a given prompt. The code is designed to run in **Google Colab** without requiring file uploads â€” everything is embedded directly.

---

## ğŸš€ Project Highlights

- âœ… Fine-tunes GPT-2 using a custom embedded dataset.
- âœ… Uses Hugging Face Transformers & Datasets.
- âœ… Runs fully in Google Colab with GPU acceleration.
- âœ… Text generation with sampling (top-k, top-p, temperature).
- âœ… No external files or API keys required.

---

## ğŸ“ Folder Structure

GPT2_Text_Generation/
â”œâ”€â”€ gpt2_finetune.ipynb # Colab-compatible notebook
â””â”€â”€ README.md # This file

---

## ğŸ“š Dependencies

Install these libraries inside Google Colab:

```python
!pip install transformers datasets accelerate
